{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING OF MULTI LAYER SVM LOSS  WITH RELU ACTIVATION FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerSVM:\n",
    "    def __init__(self, num_layers, neurons_per_layer):\n",
    "        self.num_layers = num_layers\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.LR = None\n",
    "        self.num_iters = None\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.forward_logits = []\n",
    "        self.lderivatives = []\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def relu_backward(self, dout, derivative):\n",
    "        return dout * derivative\n",
    "\n",
    "    def forward_pass(self,X,W,b):\n",
    "        output = np.dot(X,W) + b\n",
    "        ldx = W\n",
    "        ldw = X\n",
    "        ldb = np.ones_like(b)\n",
    "        return output, ldx, ldw, ldb\n",
    "    \n",
    "    def affine_backward_pass(self,ldx,ldw,ldb,ud):\n",
    "        dx = np.dot(ud,ldx.T)\n",
    "        dw = np.dot(ud.T,ldw)\n",
    "        db = np.sum(ud,axis=0)\n",
    "        return dx, dw, db\n",
    "    \n",
    "    def SVMLoss(self, logits):\n",
    "        correct_scores = np.sum(logits * self.Y, axis=1, keepdims=True)\n",
    "        diff = np.maximum(0, logits - correct_scores + 1.5)  \n",
    "        diff[np.arange(logits.shape[0]), np.argmax(self.Y, axis=1)] = 0\n",
    "        \n",
    "        loss = np.sum(diff)\n",
    "        \n",
    "        gradient = np.zeros_like(logits)\n",
    "        gradient[diff > 0] = 1\n",
    "        gradient[np.arange(logits.shape[0]), np.argmax(self.Y, axis=1)] = -np.sum(gradient, axis=1)\n",
    "        gradient /= logits.shape[0]\n",
    "        \n",
    "        return loss, gradient\n",
    "\n",
    "    def forward_pass(self):\n",
    "        for i in range(self.num_layers):\n",
    "            logits, ldx, ldw, ldb = self.forward_pass(self.X, self.weights[i], self.biases[i])\n",
    "            if i != self.num_layers-1:\n",
    "                logits = self.relu(logits)\n",
    "                ldx = self.relu_derivative(logits)\n",
    "            self.forward_logits.append(logits)\n",
    "            self.lderivatives.append((ldx,ldw,ldb))\n",
    "        return logits\n",
    "    \n",
    "    def backward_pass(self, loss_derivative):\n",
    "        ud = loss_derivative\n",
    "        i = self.num_layers-1\n",
    "        while i > -1:\n",
    "            ldx,ldw,ldb = self.lderivatives[i]\n",
    "            if i != 0 and i != self.num_layers-1:\n",
    "                ud = self.relu_backward(ud,ldw)\n",
    "            ud, dW, db = self.affine_backward_pass(ldx,ldw,ldb,ud)\n",
    "            self.weights[i] -= self.LR * dW.reshape(self.weights[i].shape)\n",
    "            self.biases[i] -= self.LR * db\n",
    "            i = i - 1\n",
    "    \n",
    "    def Train(self, X, Y, alpha=0.01, max_iterations=100, printer=10):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.LR = alpha\n",
    "        self.num_iters = max_iterations\n",
    "\n",
    "        num_classes = Y.shape[1]\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            if i == self.num_layers - 1:\n",
    "                self.weights.append(0.01 * np.random.randn(num_features, num_classes))\n",
    "            else:\n",
    "                self.weights.append(0.01 * np.random.randn(num_features, self.neurons_per_layer[i]))\n",
    "            self.biases.append(np.zeros((1, self.neurons_per_layer[i])))\n",
    "            num_features = self.neurons_per_layer[i]\n",
    "\n",
    "        print(\"Initial Weights =\", self.weights)\n",
    "        print(\"Initial Biases =\", self.biases)\n",
    "\n",
    "        for i in range(self.num_iters):\n",
    "            logits = self.forward_pass()\n",
    "            loss, ud = self.SVMLoss(logits)\n",
    "            self.backward_pass(ud)\n",
    "            if i % printer == 0:\n",
    "                print(\"Iteration {}: Loss {}\".format(i, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights = [array([[ 0.00315003, -0.00746229,  0.00298828],\n",
      "       [ 0.00661808, -0.01312527,  0.00067078],\n",
      "       [-0.01306129, -0.0049745 , -0.02721426],\n",
      "       [-0.00309587,  0.01137068, -0.00050199]])]\n",
      "Initial Biases = [array([[0., 0., 0.]])]\n",
      "Iteration 0: Loss 359.1599899850129\n",
      "Iteration 100: Loss 270.2401529965622\n",
      "Iteration 200: Loss 264.45365357166213\n",
      "Iteration 300: Loss 262.3877517536679\n",
      "Iteration 400: Loss 254.07195151527853\n",
      "Iteration 500: Loss 243.1249106483762\n",
      "Iteration 600: Loss 226.47654371026212\n",
      "Iteration 700: Loss 207.58190050330668\n",
      "Iteration 800: Loss 190.08942241763924\n",
      "Iteration 900: Loss 172.8547695382083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "encoder = LabelBinarizer()\n",
    "y_one_hot = encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "num_layers = 1\n",
    "num_classes = y_train.shape[1]  \n",
    "neurons_per_layer = [num_classes]\n",
    "model = class MultiLayerSVM(num_layers, neurons_per_layer)\n",
    "\n",
    "alpha = 0.01\n",
    "max_iterations = 1000\n",
    "printer = 100\n",
    "\n",
    "# Train the model\n",
    "model.Train(X_train, y_train, alpha, max_iterations, printer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
